{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f46f24f",
   "metadata": {},
   "source": [
    "# Priors, Likelihoods, Posteriors, and All That: Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d14e99",
   "metadata": {},
   "source": [
    "## Problem 1: The Curse of Dimensionality; or why self driving cars are hard.\n",
    "\n",
    "### 1a) Sampling in low dimensions\n",
    "\n",
    "Generate a sample of 100 randomly distributed points inside a 2D square. \n",
    "\n",
    "### 1b) Distances in low dimensions\n",
    "\n",
    "Calculate the pairwise distances between all of the points, in d-dimensions, for two points x_i and y_i, this quantity is\n",
    "\n",
    "$$ |x - y| = \\sqrt{\\Sigma (x - y)^2} $$\n",
    "\n",
    "### 1c) Distribution of distances in low dimensions \n",
    "\n",
    "Plot the distribution of these distances. Do you notice anything interesting? Is a \"special value\" picked out?\n",
    "\n",
    "### 1d) Extending to d-dimensions\n",
    "\n",
    "Now, let's consider the difference between picking a point located in a d-dimensional sphere vs in a d-dimensional cube. To do this, calculate and plot the the difference between the volume of a cube centered at the origin and going from [-1, 1] and the volume of a unit-radius sphere as the dimension d of the space increases. Possibly helpful formula: \n",
    "\n",
    "$$ Sphere: V_d = \\frac{\\pi^{d/2}}{\\frac{d}{2} \\Gamma(\\frac{d}{2})} R^d$$ \n",
    "\n",
    "where R is the radius of the sphere.\n",
    "\n",
    "$$ Cube: V_d = L^d $$\n",
    "\n",
    "and L is the length of a side.\n",
    "\n",
    "### 1e) Sampling in d-dimensions\n",
    "\n",
    "Now, consider the problem of sampling in high-dimensions. If we use a uniform prior on every parameter, our expectation is that we will well sample the parameter space simply by picking in a \"uniform way\". Calculate the ratio of volume contained in an annulus between $R$ and $R - \\epsilon$ for d = 1-20 and plot this. Interpret your result in terms of a sampling problem. Are uniform priors a good choice in higher dimensions? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a) Random points in 2D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y = np.random.rand() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116db9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ce5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) calculate pairwise distances for x, y\n",
    "\n",
    "def distance(pt1, pt2):\n",
    "    return np.sqrt((pt1[0] - pt2[0])**2 + (pt1[1] - pt2[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de65b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_list = []\n",
    "\n",
    "for pt1 in np.array(list(zip(x,y))):\n",
    "    for pt2 in np.array(list(zip(x,y))):\n",
    "        # you fill \n",
    "        \n",
    "#you fill \n",
    "plt.xlabel('distances')\n",
    "plt.ylabel('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma \n",
    "\n",
    "def V_d_sphere(d):\n",
    "    return # fill\n",
    "\n",
    "def V_d_cube(d):\n",
    "    return # fill \n",
    "\n",
    "dimensions = np.linspace(0, 20)\n",
    "\n",
    "for d in dimensions:\n",
    "    # you fill \n",
    "    \n",
    "# you fill \n",
    "plt.ylabel('$V_{sphere}$/$V_{cube}$', fontsize = 20)\n",
    "plt.xlabel('dimensions', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after some pen and paper math, it becomes clear that this ratio is simply (1-\\epsilon)^d\n",
    "\n",
    "def ratio(d, epsilon):\n",
    "    # you fill \n",
    "\n",
    "ratio_list = []\n",
    "\n",
    "for d in dimensions: \n",
    "    # you fill \n",
    "    \n",
    "# you fill \n",
    "plt.ylabel('Fractional Volume in Annulus', fontsize = 15)\n",
    "plt.xlabel('dimensions', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9872bf9",
   "metadata": {},
   "source": [
    "## Problem 2: Not everything is Gaussian\n",
    "\n",
    "In the lecture and challenge problems, we've emphasized the importance of the Gaussian distribution. This distribution is very common, but not universal. In a (perhaps apocryphal story) this problem was given to first year students at Cambridge in the 1980s. It concerns estimating the distance to an off-shore lighthouse based on the timing of pulses. A canonical statement of the problem is, \n",
    "\n",
    "\"A lighthouse is situated at unknown coordinates $x_0,y_0$ with respect to a straight coastline y=0. It sends a series of N flashes in random directions, and these are recorded on the coastline at positions $x_i$.\" \n",
    "\n",
    "### Problem 2-0) Draw the picture. \n",
    "\n",
    "With your partner, draw the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminaries \n",
    "\n",
    "from numpy.random import uniform \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d44c39",
   "metadata": {},
   "source": [
    "### Problem 2a) Prior and Data Generation \n",
    "\n",
    "Write down a prior for the $x_0, y_0$ position of the lighthouse. Implement a python function that returns a uniform probability (or if you'd like, another prior) for $x_0, y_0$. Hint: It is easier to write the prior in terms of the angle $\\theta$ between the line connecting the lighthouse to the shore and the direction in which the pulse is emitted. \n",
    "\n",
    "In order to produce a Bayesian estimate of the x-y position, we'll need some data. Generate ~$10^3$ datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unknown true values of the position \n",
    "\n",
    "x_0 = 10\n",
    "y_0  = 30\n",
    "\n",
    "# generate data assuming a uniform prior \n",
    "\n",
    "thetas = # you fill \n",
    "\n",
    "def make_data():\n",
    "    # function to return x positions at which pulses from the lighthouse are observed along the coast \n",
    "    return # you fill \n",
    "\n",
    "xs = make_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43173e9b",
   "metadata": {},
   "source": [
    "### Problem 2b) Likelihood \n",
    "\n",
    "Now, we need to determine the form of the likelihood. If you following the hint in 3a), we want to turn a function of the data (shoreline positions, x) in terms of the angle ($\\theta$). First, write down the relationship between the $x_0$ position of the lighthouse, the data x, and the angle ($\\theta$). If you get stuck here, ask us and we will provide the likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84377f74",
   "metadata": {},
   "source": [
    "### Problem 2c)\n",
    "\n",
    "Using Bayes' theorem, write down the posterior. Implement a grid search function to calculate the posterior for the x-y position of the lighthouse. For now, you can assume that the posterior is equal to the likelihood. We will shortly (jargon) **regularize** the likelihood with a prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_posterior(x_0, y_0, x): #priors will go here):\n",
    "    \n",
    "    # add priors here after completing the next step first. \n",
    "    \n",
    "    y_0 = np.abs(y_0) + 1e-10  # avoid divide-by-zero\n",
    "    log_likelihood = # you fill\n",
    "    return np.sum(log_likelihood)\n",
    "\n",
    "x_0_grid = #you fill\n",
    "y_0_grid = # you fill "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a63a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(x_0_grid, y_0_grid, x):\n",
    "    M, B = np.meshgrid() \n",
    "    posterior_grid = np.zeros(M.shape)\n",
    "\n",
    "    for #you fill:\n",
    "        for #you fill:\n",
    "            #you fill\n",
    "    \n",
    "    return posterior_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e14920",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_posterior = \n",
    "# try renormalizing here if you have numerical issues. \n",
    "posterior = \n",
    "\n",
    "plt.imshow(posterior.T,\n",
    "           extent=(x_0_grid[0], x_0_grid[-1], y_0_grid[0], y_0_grid[-1]),\n",
    "           origin='lower',\n",
    "           cmap='plasma')\n",
    "plt.plot(x_0, y_0, label='Truth')\n",
    "plt.colorbar(label='Posterior')\n",
    "plt.xlabel('x_0')\n",
    "plt.ylabel('y_0')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc7d67",
   "metadata": {},
   "source": [
    "### Problem 2d-2) Priors\n",
    "\n",
    "Now consider the effect of a prior by implementing uniform priors. Does the quality of your fit change? What if you fit with fewer datapoints, say N=100 rather than N=1000?\n",
    "\n",
    "\n",
    "### Problem 2e) [Optional] Challenge: Is your likelihood a Gaussian? What is special about your likelihood?\n",
    "\n",
    "One property of a Gaussian that makes it \"special\" is that it is the maximum entropy distribution for for finite first and second moments. Calculate the first and second moments of your likelihood distribution. What makes this distribution special? Generate some example plots of your likelihood function and compare to a Gaussian distribution over the same range. Are these curves the same? What is different about them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75849dfe",
   "metadata": {},
   "source": [
    "## Problem 3: The Hubble Tension and Measurement Inconsistency \n",
    "\n",
    "One of the most exciting topics in observational cosmology right now is the apparent inconsistency in measurements of the hubble constant from early and late times in the history of the universe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e62425a",
   "metadata": {},
   "source": [
    "### Preliminaries - import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e4bdc",
   "metadata": {},
   "source": [
    "### Problem 3a) Generate two toy datasets with different values of the slope and intercept to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80992b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "def model(m, b, x):\n",
    "    return m*x + b\n",
    "\n",
    "x = np.linspace(7, 12)\n",
    "y = model(1.24, -10.09, x)\n",
    "y2 = model(2, -9.5, x)\n",
    "\n",
    "yerr = #make up some errors\n",
    "data_y = y + #include scatter \n",
    "\n",
    "yerr2 = #make up some errors\n",
    "data_y2 = y2 + #include scatter\n",
    "\n",
    "\n",
    "# make a plot \n",
    "\n",
    "plt.xlabel('Mass ($M_{Sun}$)')\n",
    "plt.ylabel('SFR ($M_{Sun} yr^{-1}$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cabd528",
   "metadata": {},
   "source": [
    "### Problem 3b) Use the MAP estimate for the first line to initialize an emcee sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [1.28, -10.4] + 0.1*np.random.randn(32, 2)\n",
    "nwalkers, ndim = pos.shape\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers, ndim, log_probability, args=(x, data_y, yerr)\n",
    ")\n",
    "\n",
    "sampler.run_mcmc(pos, #how long should this run for \n",
    "                 , progress=True)\n",
    "\n",
    "import corner\n",
    "flat_samples = sampler.get_chain(#how many values should be discarded as \"burn in\", \n",
    "                                 thin=100, flat=True)\n",
    "\n",
    "\n",
    "labels = [\"m\", \"b\"]\n",
    "\n",
    "fig = corner.corner(\n",
    "    flat_samples, labels=labels, truths=[1.24, -10.09], quantiles = [0.05, 0.5, 0.95]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b59969",
   "metadata": {},
   "source": [
    "### Problem 3c) Now fit the combination of both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356bb73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make total arrays that have the combined dataset \n",
    "# discusss with your neighbor how to do a joint fit of two datasets - should they be fit together\n",
    "# or separately? \n",
    "\n",
    "pos = [1.28, -10.4] + 0.1*np.random.randn(32, 2)\n",
    "nwalkers, ndim = pos.shape\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers, ndim, log_probability, args=(x_total, y_total, yerr_total)\n",
    ")\n",
    "\n",
    "sampler.run_mcmc(pos, #how many steps to run?\n",
    "                 , progress=True)\n",
    "\n",
    "flat_samples2 = sampler.get_chain(discard= #how many to discard\n",
    "                                  , thin=100, flat=True)\n",
    "\n",
    "labels = [\"m\", \"b\"]\n",
    "\n",
    "fig = corner.corner(\n",
    "    flat_samples2, labels=labels, truths=[1.24, -10.09], quantiles = [0.05, 0.5, 0.95]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4344f4d",
   "metadata": {},
   "source": [
    "## Problem 4: Measuring distances - when priors matter\n",
    "\n",
    "A famous example where your choice of prior matters is found in parallax measurements. In this problem, we will explore this. \n",
    "\n",
    "(Thank you Adrian Price-Whelan for contributing this problem!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "\n",
    "import astropy.units as u\n",
    "import astropy.table as at\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# import arviz as az\n",
    "from astroquery.gaia import Gaia\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbbe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query Gaia \n",
    "# Cohort 5 students should confirm that they understand the database syntax here based on \n",
    "# the material from Session 15.\n",
    "# Cohort 6 students - we will cover this in Session 21 : ) \n",
    "\n",
    "job = Gaia.launch_job(\n",
    "    \"SELECT TOP 1 * FROM gaiadr3.gaia_source WHERE parallax_over_error > 3 and parallax_over_error < 4\"\n",
    ")\n",
    "data = at.QTable(job.get_results().filled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60690f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the data - make sure you understand its structure\n",
    "\n",
    "# you fill "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's consider two choices of prior. First, a uniform prior, and then a truncated normal. \n",
    "\n",
    "def uniform_space_density_logp(L):\n",
    "    def lpdf(r):\n",
    "        return (2*pm.math.log(r)) - 3*pm.math.log(L) - pm.math.log(2) - r/L\n",
    "    return lpdf\n",
    "\n",
    "with pm.Model() as model:\n",
    "    r = pm.DensityDist(\n",
    "        'r', \n",
    "        logp=uniform_space_density_logp(4.), \n",
    "        initval=1.,\n",
    "        transform=pm.distributions.transforms.LogTransform()\n",
    "    )\n",
    "    plx = pm.Normal(\n",
    "        'plx',\n",
    "        mu=1/r,\n",
    "        sigma=data['parallax_error'][0].to_value(u.mas),\n",
    "        observed=data['parallax'][0].to_value(u.mas)\n",
    "    )\n",
    "    \n",
    "    samples_usd = pm.sample(tune=2000, draws=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    r = pm.TruncatedNormal(\n",
    "        'r', \n",
    "        mu=2.,\n",
    "        sigma=1.,\n",
    "        lower=0,\n",
    "        initval=1.,\n",
    "        transform=pm.distributions.transforms.LogTransform()\n",
    "    )\n",
    "    plx = pm.Normal(\n",
    "        'plx',\n",
    "        mu=1/r,\n",
    "        sigma=data['parallax_error'][0].to_value(u.mas),\n",
    "        observed=data['parallax'][0].to_value(u.mas)\n",
    "    )\n",
    "    \n",
    "    samples_truncnorm = pm.sample(tune=2000, draws=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe69ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And plot the results\n",
    "\n",
    "# you fill "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ee7b5",
   "metadata": {},
   "source": [
    "## [Pen & Paper] Challenge Problem: The Gaussian Derivation of the Central Distribution\n",
    "\n",
    "In the lecture, we see two derivations of the Central distribution based on physical arguments. The first, spatial homogeneity, arose to solve a problem in astronomy, while the second based on convolutions of distributions (and \"additive physical processes\" arose in condensed matter physics and electrical engineering. In this problem, we'll study another derivation that also arose in the fitting of the orbits of the planets. \n",
    "\n",
    "### Maximum Likelihood Estimates\n",
    "\n",
    "Begin by reviewing the discussion in Adam's lecture yesterday of the \"maximum likelihood estimate\" of a parameter. Then, write a general expression for the maximum of the log-likelihood. Then re-express your MLE statmeent in terms of a function $g'(\\hat{\\theta} - x$), where $\\hat{\\theta}$ is your MLE estimate for the parameter $\\theta$ and $g(\\theta-x) = log f(x_i | \\theta)$ is your log-likelihood.\n",
    "\n",
    "### The arithmetic mean and roots of the MLE\n",
    "\n",
    "Now assume that the MLE must be given by the arithmetic mean of the observations, \n",
    "\n",
    "$$ \\hat{\\theta} = \\bar{x} = \\frac{1}{n+1} \\Sigma_{i=0}^n x_i $$ and consider a simple sample. This sample should have one observation $x_0 = (n+1)(\\theta - x)$. Now compute $\\hat{\\theta}$ and $\\hat{\\theta} - x_0$. What is the value of $g'(\\hat{\\theta} - x$)? Is this symmetric or anti-symmetric?\n",
    "\n",
    "### Functional equations\n",
    "\n",
    "Given your expression for $g'(\\hat{\\theta} - x)$, do some functional analysis. What are the possible functional forms of $g(u)$? Then plug them into your original MLE expression. You will find that \n",
    "\n",
    "$$ f(x|\\theta) = \\sqrt{\\frac{\\alpha}{2\\pi}} \\exp \\left[ -  \\frac{1}{2} \\alpha (x-\\theta)^2 \\right]$$\n",
    "\n",
    "As noted in the lecture, the historical importance of this result is that the assumption that $\\hat{\\theta} = \\bar{x}$ provides a theoretical basis for the intuitive notion that errors cancel. This put to rest a long running argument about the nature of additive errors and justifies much of what we assume as a matter of course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
