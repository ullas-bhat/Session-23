{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bYBRseluX7_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals for the Lecture\n",
    "\n",
    "The learning goal for this lesson is to provide some background on Machine Learning that we'll use in Sessions 23-26, and to (start to) answer the following questions:\n",
    "\n",
    "- What is Machine Learning? How is different from statistics?\n",
    "- Where did the field come from? Where is it going?\n",
    "- When do we use Machine Learning and what are its limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HL5kOQtumMc",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "REP6Xd-vuZti",
    "outputId": "727fa7c9-9126-4b79-d39f-ea939465c7d7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('aygSMgK3BEM', width=1200, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "qLzZ6YXdufHq",
    "outputId": "9e0c63c1-c3e1-45f1-fdf7-990925c1e1b5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('cNxadbrN_aI', width=1200, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZcL4P65u1dr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When do we need machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZcL4P65u1dr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, when a task is to complex to perform. That is, where it is\n",
    "- hard to specify an algorithm for solving the problem\n",
    "- the data is very large or complex\n",
    "\n",
    "Second, tasks that require adaptivity\n",
    "- where a task must change as a result of interaction with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "kXjEMSoQu04T",
    "outputId": "caaa5b6a-4414-4963-cbaf-bfa9d89231b6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('zMpEeag7kkM', width=1200, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyTVirbhvKhD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The (Formal) Structure of the Learning Problem\n",
    "\n",
    "A Learning Problem consists of the following parts\n",
    "\n",
    "$\\mathit{X}$ - a domain set or instance space of examples. These are usually n-dimensional vectors. We call the components of these vectors, $\\textit{features}$.\n",
    "\n",
    "$\\mathit{Y}$ - the label set, in supervised learning problems these are the set of possible $\\textit{labels}$ for each element in $\\mathit{X}$.\n",
    "\n",
    "$\\mathit{S}$ - the training set. These are ordered pairs of elements from $\\mathit{X}$ and $\\mathit{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rAWBAYkvb-f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For many problems, we additionally assume there is some true mapping $f: \\mathit{X} \\rightarrow \\mathit{Y}$. These problems are called $\\textit{supervised learning}$. If no true mapping exists, this is called an $\\textit{unsupervised learning}$ problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ53899nxFVK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using the [ten hundred most common words](https://xkcd.com/simplewriter/) in the English language$^\\dagger$, I would describe unsupervised learning as\n",
    "\n",
    "> learning information without class names\n",
    "\n",
    "or\n",
    "\n",
    "> finding hidden breaks between groups\n",
    "\n",
    "$^\\dagger$ See [Up-goer five](https://xkcd.com/1133/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ53899nxFVK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we expand our volabulary slightly, then unsupervised learning could be described as\n",
    "\n",
    "> the search for underlying structure within the data\n",
    "\n",
    "or\n",
    "\n",
    "> learning from data without rewards (reinforcement learning) or labels (supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ53899nxFVK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What follows is a broad overview of several of the common techniques that constitute unsupervised learning.\n",
    "\n",
    "We start with a familiar example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ53899nxFVK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose Nature produces some family of sources that tend to be either big or small. Mathematically we might represent the distribution of these sources as\n",
    "\n",
    "$$P \\propto 0.35\\mathcal{N}(30, 5^2) + 0.65\\mathcal{N}(60,8^2)$$\n",
    "\n",
    "where $\\mathcal{N}$ represents the normal distribution.\n",
    "\n",
    "Using [`np.random`](https://numpy.org/doc/stable/reference/random/index.html) we can generate samples from this distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBGcOv5fvcXZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def generate_samples(n_sources):\n",
    "    rand_dist = np.random.rand(n_sources)\n",
    "    size = np.empty_like(rand_dist)\n",
    "    small = rand_dist <= 0.35\n",
    "    size[small] = np.random.normal(30, 5, size=sum(small))\n",
    "    size[~small] = np.random.normal(60,8,  size=sum(~small))\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Enh0bKp-xmGz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In practice, we observe nature and do not know the true probability density function (PDF) for the sources of interest.\n",
    "\n",
    "The subject of density estimation (of this PDF) will be a topic that we explore, from a statistical perspective, thoroughly this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0POqopOXypP5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering\n",
    "\n",
    "Closely related to density estimation is clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0POqopOXypP5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0POqopOXypP5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Every discussion of clustering starts with K-means, which is a simple method to identify clusters within a dataset.\n",
    "\n",
    "In brief, the user specifies the number of clusters to search for $K$, and the algorithm proceeds to divide sources in the feature space accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0POqopOXypP5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The pseudocode is:  \n",
    "\n",
    "  1. choose the number of clusters, K\n",
    "  2. choose K random sources for initial cluster centers\n",
    "  3. assign each source to the nearest cluster (min distance between cluster center and source)\n",
    "  4. update the position of the cluster center to be the mean position of all in-cluster sources\n",
    "  5. repeat steps 3 and 4\n",
    "\n",
    "Once a user-specified stopping criteria is reached, such as $N$ cluster updates or some small fractional change in the cluster centers, further updates do not occur.\n",
    "\n",
    "We can visually demonstrate how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "LoRl0UZMxkIq",
    "outputId": "97ebce46-6615-47f3-f6bd-da7089fc0a4e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs([50,100,100], n_features=2, cluster_std=1, center_box=(-4,4), random_state=23)\n",
    "\n",
    "def kmeans_init(ax, fs=15):\n",
    "    ax.scatter(X[:,0], X[:,1], facecolor=\"None\", edgecolors='k')\n",
    "    ax.scatter(X[0:3,0], X[0:3,1],\n",
    "           marker='X', s=500, c=[0,1,2], edgecolors='0.7')\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "kmeans_init(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnrXeENYy8eq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_assign(ax, X=X, fs=15):\n",
    "    clf = KMeans(n_clusters=3, init = X[0:3], max_iter=1, n_init=1)\n",
    "    clf.fit(X)\n",
    "\n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    colors = cmap(clf.labels_/2)\n",
    "\n",
    "    ax.scatter(X[:,0], X[:,1], facecolor=\"None\", edgecolors=colors)\n",
    "    ax.scatter(X[0:3,0], X[0:3,1],\n",
    "           marker='X', s=500, c=[0,1,2], edgecolors='0.7')\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "kmeans_assign(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "no6ev-y8zjcP",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Write an iterative plotting showing how the cluster centers update. \n",
    "### Do this for one iteration, and then for the tenth iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJ9n6vUezEc3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clf = # fill in \n",
    "clf.fit() # fill in \n",
    "\n",
    "cmap = plt.get_cmap(\"viridis\")\n",
    "colors = cmap(clf.labels_/2)\n",
    "\n",
    "center1 = \n",
    "\n",
    "def kmeans1(ax, X=X, fs=15):\n",
    "\n",
    "\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBR73i1TzpgI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "# fill in \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = []\n",
    "center.append()\n",
    "\n",
    "\n",
    "\n",
    "def kmeans1(ax, X=X, fs=15):\n",
    "\n",
    "    alphas = 0.1 * np.ones(9);\n",
    "    alphas = np.hstack((alphas,1))\n",
    "\n",
    "    # fill in \n",
    "        \n",
    "    return alphas\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "# fill in \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nim62bKnzv5s",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this (extremely) simple example we can see that the clusters have converged.\n",
    "\n",
    "Under the hood, there are some user-defined parameters (e.g., distance metric â€“ we used Euclidean), but one choice stands above all else..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nim62bKnzv5s",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How in the world do we choose $K$?\n",
    "\n",
    "For the previous example, we created the data and therefore knew $K = 3$ was reasonable. When clustering new data, however, $K$ is truly unknown. By definition the algorithm will always find $K$ clusters, whether or not there are $K$ clusters present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nim62bKnzv5s",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many clustering methods require the user to specify the number of clusters, prior to the application of the algorithm. There is also no natural metric for calculating if the specified number of clusters is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agXo_ejR0HIv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# let's tackle a slightly more complicated problem: k-nn classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3c8779a7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('star_classification.csv', sep = ',')\n",
    "\n",
    "df = data.sample(frac = 0.2, random_state = 11)\n",
    "\n",
    "df.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0698ed69",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's learn a few things about this data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35d86865",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# look at the columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4024fdef",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can get some basic info about the data (numerical columns only!) like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89a889e2",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# use a pandas method to look at the statistics describing this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd0394ca",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What should be our features and targets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2966bc8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c844f607",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can select the following columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f34b2d04",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# select the features \n",
    "\n",
    "seldf = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1097580",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature exploration.\n",
    "\n",
    "What should we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b6b31ed",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2571d100",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# make a histogram of the selected features\n",
    "for col in :\n",
    "    plt.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1395173",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When a histogram looks like this, what does it mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbb27ce2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# fix the issue(s) with the previous histogram \n",
    "\n",
    "for col in :\n",
    "    plt.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "629f0fe9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# what issue did you identify and where do you think \n",
    "# this issue comes from in terms of how photometry is done? \n",
    "# print out the number of problematic measurments for each feature\n",
    "\n",
    "for col in:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98936b68",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One possible (less than ideal) selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce70af18",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for col in :\n",
    "    seldf = # fill in  >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "658e31e8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "seldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f69d081a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for col in :\n",
    "    plt.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79e6f958",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now turn to the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99855784",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# print out the target categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2467d80e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# use the sklearn LabelEncoder to make labels for these categories \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = #fill in, turns categorical into 1 ... N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2214f779",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# apply the labels \n",
    "\n",
    "y = # fill in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c376da17",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# print out the classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70bd2f44",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is also useful to know which class was mapped to which number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93fde9c7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# dictionary \n",
    "\n",
    "le_name_mapping = # print the mapping as a dictionary\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c01ff17",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now plot the class distribution to see if something stands out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "876027b9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# make a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f514d070",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Verdict: Is there anything we should keep in mind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29db7723",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63e3a03b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "target = y[seldf.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e2db9de",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have our feature matrix and target vector, we could try and build a model, or try to get some more intel on the relationship between features and target. One way is to build a correlation matrix (easiest with seaborn); another one that I like when there are very few features like in this case, is to make scatter plots of the features and color the points by the label, to see if any obvious gradient shows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d734eac1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "len(seldf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7f0c99c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#It will take a little time.\n",
    "\n",
    "fig, axes = plt.subplots(ncols = len(seldf.columns), nrows = len(seldf.columns), figsize=(50,50))\n",
    "\n",
    "for i, col in enumerate(seldf.columns):\n",
    "    for j, col2 in enumerate(seldf.columns):\n",
    "        axes[i,j].scatter(seldf[col], seldf[col2], c = target)\n",
    "        axes[i,j].set_xlabel(str(seldf[col].name), fontsize = 35)\n",
    "        axes[i,j].set_ylabel(str(seldf[col2].name), fontsize = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d809085d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q: Is there anything special that we can learn from these plots? Do we expect our features to have enough discriminating power? Why or why not?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28d3cc6b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d79ea23",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e14f579b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alright, time to build our first model - let's start with kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec0fcf45",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This line helps visualize the parameters of any algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b9231a9"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# fill in, define the model and print out the parameters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c97bfe3"
   },
   "source": [
    "We can set up a cross validation strategy like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8383c370"
   },
   "outputs": [],
   "source": [
    "# now perform cross validation \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=10, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eb8d9f8"
   },
   "source": [
    "This line does the cross validation for us by randomizing the data, creating five folds, building 5 models with the 5 possible training sets, applying each model to the objects in the test fold, and saving the 5 train and test scores in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed312748"
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(model, seldf, target, cv = cv, return_train_score = True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3304513",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Time: %.3fs, Mean train score: %.3f, Mean test score: %.3f, std: %.3f\"%(scores['score_time'].mean(), scores['train_score'].mean(), scores['test_score'].mean(), scores['test_score'].std()))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
